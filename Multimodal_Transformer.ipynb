{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMThv6RQSPlR6fTMonAkBLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nid989/Multimodal-Transformer/blob/master/Multimodal_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "link to the MOSEI Sentiment Analysis datasey [here](https://paperswithcode.com/dataset/cmu-mosei)"
      ],
      "metadata": {
        "id": "QtPPloCAvvBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmb4i9Zji_lH",
        "outputId": "ba0d6690-33fe-4d45-88f2-6528c8df5534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Multimodal-Transformer'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 83 (delta 35), reused 51 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (83/83), done.\n"
          ]
        }
      ],
      "source": [
        "# clone repository\n",
        "# !git clone https://github.com/yaohungt/Multimodal-Transformer.git\n",
        "!git clone https://github.com/Nid989/Multimodal-Transformer.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change root directory\n",
        "%cd Multimodal-Transformer/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OClk51Wjn1Mq",
        "outputId": "7bc4daaa-2fbc-4e78-a13d-865fd7c6346d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Multimodal-Transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data pre_trained_models"
      ],
      "metadata": {
        "id": "y825NQkZlS_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd data # change directory to ./data\n",
        "wget https://www.dropbox.com/sh/hyzpgx1hp9nj37s/AADfY2s7gD_MkR76m03KS0K1a/Archive.zip?dl=1\n",
        "mv 'Archive.zip?dl=1' Archive.zip\n",
        "unzip Archive.zip\n",
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Tcgh5D-kujN",
        "outputId": "a5dfaf6d-f3a6-44ce-fbb9-5eba420783b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Archive.zip\n",
            "  inflating: mosei_senti_data.pkl    \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._mosei_senti_data.pkl  \n",
            "  inflating: iemocap_data.pkl        \n",
            "  inflating: __MACOSX/._iemocap_data.pkl  \n",
            "  inflating: mosei_senti_data_noalign.pkl  \n",
            "  inflating: __MACOSX/._mosei_senti_data_noalign.pkl  \n",
            "  inflating: mosi_data.pkl           \n",
            "  inflating: __MACOSX/._mosi_data.pkl  \n",
            "  inflating: mosi_data_noalign.pkl   \n",
            "  inflating: __MACOSX/._mosi_data_noalign.pkl  \n",
            "  inflating: iemocap_data_noalign.pkl  \n",
            "  inflating: __MACOSX/._iemocap_data_noalign.pkl  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --lonly --batch_size 2 --num_epochs 1 --aligned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD11vvLqmsFO",
        "outputId": "4712c204-061b-47a0-ed7c-9999d462e7dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset: mosei_senti\n",
            "valid_partial_mode: 1\n",
            "use_cuda: True\n",
            "Start loading the data....\n",
            "getting train data....\n",
            "  - Found cached train data\n",
            "getting validation(valid) data....\n",
            "  - Found cached valid data\n",
            "getting test data....\n",
            "  - Found cached test data\n",
            "using DataLoader for train data....\n",
            "using DataLoader for validation(valid) data....\n",
            "using DataLoader for test data....\n",
            "Finish loading the data....\n",
            "initialising hyperparameters and assign arguments....\n",
            "cuda:0\n",
            "optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "criterion: L1Loss()\n",
            "initialising training parameters for aligned data....\n",
            "scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f588b8a41d0>\n",
            "training args/config: {'model': MULTModel(\n",
            "  (proj_l): Conv1d(300, 30, kernel_size=(1,), stride=(1,), bias=False)\n",
            "  (proj_a): Conv1d(74, 30, kernel_size=(1,), stride=(1,), bias=False)\n",
            "  (proj_v): Conv1d(35, 30, kernel_size=(1,), stride=(1,), bias=False)\n",
            "  (trans_l_with_a): TransformerEncoder(\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=30, out_features=30, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=30, out_features=120, bias=True)\n",
            "        (fc2): Linear(in_features=120, out_features=30, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=30, out_features=30, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=30, out_features=120, bias=True)\n",
            "        (fc2): Linear(in_features=120, out_features=30, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=30, out_features=30, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=30, out_features=120, bias=True)\n",
            "        (fc2): Linear(in_features=120, out_features=30, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=30, out_features=30, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=30, out_features=120, bias=True)\n",
            "        (fc2): Linear(in_features=120, out_features=30, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=30, out_features=30, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=30, out_features=120, bias=True)\n",
            "        (fc2): Linear(in_features=120, out_features=30, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (trans_l_with_v): TransformerEncoder(\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=30, out_features=30, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=30, out_features=120, bias=True)\n",
            "        (fc2): Linear(in_features=120, out_features=30, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=30, out_features=30, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=30, out_features=120, bias=True)\n",
            "        (fc2): Linear(in_features=120, out_features=30, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=30, out_features=30, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=30, out_features=120, bias=True)\n",
            "        (fc2): Linear(in_features=120, out_features=30, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=30, out_features=30, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=30, out_features=120, bias=True)\n",
            "        (fc2): Linear(in_features=120, out_features=30, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=30, out_features=30, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=30, out_features=120, bias=True)\n",
            "        (fc2): Linear(in_features=120, out_features=30, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (trans_l_mem): TransformerEncoder(\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (trans_a_mem): TransformerEncoder(\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (trans_v_mem): TransformerEncoder(\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=60, out_features=240, bias=True)\n",
            "        (fc2): Linear(in_features=240, out_features=60, bias=True)\n",
            "        (layer_norms): ModuleList(\n",
            "          (0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (proj1): Linear(in_features=60, out_features=60, bias=True)\n",
            "  (proj2): Linear(in_features=60, out_features=60, bias=True)\n",
            "  (out_layer): Linear(in_features=60, out_features=1, bias=True)\n",
            "), 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            "), 'criterion': L1Loss(), 'ctc_a2l_module': None, 'ctc_v2l_module': None, 'ctc_a2l_optimizer': None, 'ctc_v2l_optimizer': None, 'ctc_criterion': None, 'scheduler': <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f588b8a41d0>}\n",
            "start train procedure....\n",
            "setting model to train config....\n",
            "num_batches: 8132\n",
            "start_time: 1662919419.9588838\n",
            "Epoch  1 | Batch  30/8132 | Time/Batch(ms) 120.91 | Train Loss 1.1862\n",
            "Epoch  1 | Batch  60/8132 | Time/Batch(ms) 64.57 | Train Loss 0.8542\n",
            "Epoch  1 | Batch  90/8132 | Time/Batch(ms) 64.06 | Train Loss 1.0597\n",
            "Epoch  1 | Batch 120/8132 | Time/Batch(ms) 62.97 | Train Loss 0.7444\n",
            "Epoch  1 | Batch 150/8132 | Time/Batch(ms) 63.04 | Train Loss 0.9939\n",
            "Epoch  1 | Batch 180/8132 | Time/Batch(ms) 64.14 | Train Loss 1.0377\n",
            "Epoch  1 | Batch 210/8132 | Time/Batch(ms) 61.87 | Train Loss 0.8161\n",
            "Epoch  1 | Batch 240/8132 | Time/Batch(ms) 60.94 | Train Loss 0.9131\n",
            "Epoch  1 | Batch 270/8132 | Time/Batch(ms) 61.71 | Train Loss 0.8611\n",
            "Epoch  1 | Batch 300/8132 | Time/Batch(ms) 62.07 | Train Loss 0.9014\n",
            "Epoch  1 | Batch 330/8132 | Time/Batch(ms) 64.96 | Train Loss 0.7570\n",
            "Epoch  1 | Batch 360/8132 | Time/Batch(ms) 62.42 | Train Loss 0.9916\n",
            "Epoch  1 | Batch 390/8132 | Time/Batch(ms) 61.91 | Train Loss 0.8520\n",
            "Epoch  1 | Batch 420/8132 | Time/Batch(ms) 61.53 | Train Loss 0.8965\n",
            "Epoch  1 | Batch 450/8132 | Time/Batch(ms) 64.28 | Train Loss 0.9483\n",
            "Epoch  1 | Batch 480/8132 | Time/Batch(ms) 62.51 | Train Loss 0.7644\n",
            "Epoch  1 | Batch 510/8132 | Time/Batch(ms) 62.40 | Train Loss 0.7729\n",
            "Epoch  1 | Batch 540/8132 | Time/Batch(ms) 62.34 | Train Loss 0.8400\n",
            "Epoch  1 | Batch 570/8132 | Time/Batch(ms) 61.85 | Train Loss 1.0103\n",
            "Epoch  1 | Batch 600/8132 | Time/Batch(ms) 61.54 | Train Loss 0.9824\n",
            "Epoch  1 | Batch 630/8132 | Time/Batch(ms) 61.76 | Train Loss 0.8061\n",
            "Epoch  1 | Batch 660/8132 | Time/Batch(ms) 61.64 | Train Loss 0.6947\n",
            "Epoch  1 | Batch 690/8132 | Time/Batch(ms) 62.33 | Train Loss 0.9662\n",
            "Epoch  1 | Batch 720/8132 | Time/Batch(ms) 62.47 | Train Loss 0.9145\n",
            "Epoch  1 | Batch 750/8132 | Time/Batch(ms) 61.40 | Train Loss 0.9819\n",
            "Epoch  1 | Batch 780/8132 | Time/Batch(ms) 61.10 | Train Loss 0.8243\n",
            "Epoch  1 | Batch 810/8132 | Time/Batch(ms) 62.02 | Train Loss 1.0470\n",
            "Epoch  1 | Batch 840/8132 | Time/Batch(ms) 62.54 | Train Loss 0.8653\n",
            "Epoch  1 | Batch 870/8132 | Time/Batch(ms) 62.31 | Train Loss 0.8155\n",
            "Epoch  1 | Batch 900/8132 | Time/Batch(ms) 61.91 | Train Loss 0.9075\n",
            "Epoch  1 | Batch 930/8132 | Time/Batch(ms) 61.88 | Train Loss 0.8477\n",
            "Epoch  1 | Batch 960/8132 | Time/Batch(ms) 62.41 | Train Loss 0.9801\n",
            "Epoch  1 | Batch 990/8132 | Time/Batch(ms) 63.08 | Train Loss 0.7915\n",
            "Epoch  1 | Batch 1020/8132 | Time/Batch(ms) 62.15 | Train Loss 0.9010\n",
            "Epoch  1 | Batch 1050/8132 | Time/Batch(ms) 61.25 | Train Loss 0.8378\n",
            "Epoch  1 | Batch 1080/8132 | Time/Batch(ms) 62.43 | Train Loss 0.9577\n",
            "Epoch  1 | Batch 1110/8132 | Time/Batch(ms) 62.58 | Train Loss 0.9884\n",
            "Epoch  1 | Batch 1140/8132 | Time/Batch(ms) 61.02 | Train Loss 0.9463\n",
            "Epoch  1 | Batch 1170/8132 | Time/Batch(ms) 62.14 | Train Loss 0.8860\n",
            "Epoch  1 | Batch 1200/8132 | Time/Batch(ms) 61.79 | Train Loss 0.8454\n",
            "Epoch  1 | Batch 1230/8132 | Time/Batch(ms) 62.23 | Train Loss 1.0683\n",
            "Epoch  1 | Batch 1260/8132 | Time/Batch(ms) 62.56 | Train Loss 0.7537\n",
            "Epoch  1 | Batch 1290/8132 | Time/Batch(ms) 62.78 | Train Loss 0.8481\n",
            "Epoch  1 | Batch 1320/8132 | Time/Batch(ms) 62.36 | Train Loss 0.9399\n",
            "Epoch  1 | Batch 1350/8132 | Time/Batch(ms) 62.84 | Train Loss 0.7690\n",
            "Epoch  1 | Batch 1380/8132 | Time/Batch(ms) 62.45 | Train Loss 0.7753\n",
            "Epoch  1 | Batch 1410/8132 | Time/Batch(ms) 63.34 | Train Loss 0.8354\n",
            "Epoch  1 | Batch 1440/8132 | Time/Batch(ms) 61.96 | Train Loss 0.9179\n",
            "Epoch  1 | Batch 1470/8132 | Time/Batch(ms) 62.07 | Train Loss 0.7677\n",
            "Epoch  1 | Batch 1500/8132 | Time/Batch(ms) 62.75 | Train Loss 0.8632\n",
            "Epoch  1 | Batch 1530/8132 | Time/Batch(ms) 61.92 | Train Loss 0.8215\n",
            "Epoch  1 | Batch 1560/8132 | Time/Batch(ms) 62.00 | Train Loss 0.9316\n",
            "Epoch  1 | Batch 1590/8132 | Time/Batch(ms) 61.75 | Train Loss 0.8801\n",
            "Epoch  1 | Batch 1620/8132 | Time/Batch(ms) 61.63 | Train Loss 0.9809\n",
            "Epoch  1 | Batch 1650/8132 | Time/Batch(ms) 62.24 | Train Loss 0.9346\n",
            "Epoch  1 | Batch 1680/8132 | Time/Batch(ms) 61.63 | Train Loss 0.6521\n",
            "Epoch  1 | Batch 1710/8132 | Time/Batch(ms) 62.03 | Train Loss 0.7775\n",
            "Epoch  1 | Batch 1740/8132 | Time/Batch(ms) 61.87 | Train Loss 0.7739\n",
            "Epoch  1 | Batch 1770/8132 | Time/Batch(ms) 61.54 | Train Loss 0.9291\n",
            "Epoch  1 | Batch 1800/8132 | Time/Batch(ms) 61.98 | Train Loss 0.6858\n",
            "Epoch  1 | Batch 1830/8132 | Time/Batch(ms) 62.23 | Train Loss 0.8474\n",
            "Epoch  1 | Batch 1860/8132 | Time/Batch(ms) 61.90 | Train Loss 0.8789\n",
            "Epoch  1 | Batch 1890/8132 | Time/Batch(ms) 62.05 | Train Loss 1.0530\n",
            "Epoch  1 | Batch 1920/8132 | Time/Batch(ms) 62.43 | Train Loss 0.7282\n",
            "Epoch  1 | Batch 1950/8132 | Time/Batch(ms) 62.66 | Train Loss 0.6858\n",
            "Epoch  1 | Batch 1980/8132 | Time/Batch(ms) 62.67 | Train Loss 0.8986\n",
            "Epoch  1 | Batch 2010/8132 | Time/Batch(ms) 63.25 | Train Loss 0.8106\n",
            "Epoch  1 | Batch 2040/8132 | Time/Batch(ms) 62.98 | Train Loss 0.7453\n",
            "Epoch  1 | Batch 2070/8132 | Time/Batch(ms) 63.53 | Train Loss 0.8795\n",
            "Epoch  1 | Batch 2100/8132 | Time/Batch(ms) 62.69 | Train Loss 0.9504\n",
            "Epoch  1 | Batch 2130/8132 | Time/Batch(ms) 62.76 | Train Loss 0.8272\n",
            "Epoch  1 | Batch 2160/8132 | Time/Batch(ms) 62.65 | Train Loss 0.8696\n",
            "Epoch  1 | Batch 2190/8132 | Time/Batch(ms) 63.21 | Train Loss 0.8079\n",
            "Epoch  1 | Batch 2220/8132 | Time/Batch(ms) 62.27 | Train Loss 0.7990\n",
            "Epoch  1 | Batch 2250/8132 | Time/Batch(ms) 62.85 | Train Loss 0.8741\n",
            "Epoch  1 | Batch 2280/8132 | Time/Batch(ms) 63.77 | Train Loss 0.8874\n",
            "Epoch  1 | Batch 2310/8132 | Time/Batch(ms) 62.57 | Train Loss 0.6909\n",
            "Epoch  1 | Batch 2340/8132 | Time/Batch(ms) 62.89 | Train Loss 0.8404\n",
            "Epoch  1 | Batch 2370/8132 | Time/Batch(ms) 63.76 | Train Loss 0.7508\n",
            "Epoch  1 | Batch 2400/8132 | Time/Batch(ms) 62.65 | Train Loss 0.7815\n",
            "Epoch  1 | Batch 2430/8132 | Time/Batch(ms) 62.78 | Train Loss 0.8775\n",
            "Epoch  1 | Batch 2460/8132 | Time/Batch(ms) 62.11 | Train Loss 0.7781\n",
            "Epoch  1 | Batch 2490/8132 | Time/Batch(ms) 62.59 | Train Loss 0.7098\n",
            "Epoch  1 | Batch 2520/8132 | Time/Batch(ms) 62.63 | Train Loss 0.7588\n",
            "Epoch  1 | Batch 2550/8132 | Time/Batch(ms) 63.92 | Train Loss 0.8330\n",
            "Epoch  1 | Batch 2580/8132 | Time/Batch(ms) 63.78 | Train Loss 0.8200\n",
            "Epoch  1 | Batch 2610/8132 | Time/Batch(ms) 63.22 | Train Loss 0.7966\n",
            "Epoch  1 | Batch 2640/8132 | Time/Batch(ms) 62.83 | Train Loss 0.9193\n",
            "Epoch  1 | Batch 2670/8132 | Time/Batch(ms) 62.38 | Train Loss 0.7775\n",
            "Epoch  1 | Batch 2700/8132 | Time/Batch(ms) 62.73 | Train Loss 0.8489\n",
            "Epoch  1 | Batch 2730/8132 | Time/Batch(ms) 63.61 | Train Loss 0.8100\n",
            "Epoch  1 | Batch 2760/8132 | Time/Batch(ms) 63.84 | Train Loss 0.8392\n",
            "Epoch  1 | Batch 2790/8132 | Time/Batch(ms) 63.96 | Train Loss 0.5788\n",
            "Epoch  1 | Batch 2820/8132 | Time/Batch(ms) 63.67 | Train Loss 0.8389\n",
            "Epoch  1 | Batch 2850/8132 | Time/Batch(ms) 63.77 | Train Loss 0.7240\n",
            "Epoch  1 | Batch 2880/8132 | Time/Batch(ms) 63.20 | Train Loss 0.8774\n",
            "Epoch  1 | Batch 2910/8132 | Time/Batch(ms) 61.95 | Train Loss 0.9482\n",
            "Epoch  1 | Batch 2940/8132 | Time/Batch(ms) 62.08 | Train Loss 0.8350\n",
            "Epoch  1 | Batch 2970/8132 | Time/Batch(ms) 62.24 | Train Loss 0.8348\n",
            "Epoch  1 | Batch 3000/8132 | Time/Batch(ms) 62.21 | Train Loss 0.8397\n",
            "Epoch  1 | Batch 3030/8132 | Time/Batch(ms) 63.54 | Train Loss 0.7834\n",
            "Epoch  1 | Batch 3060/8132 | Time/Batch(ms) 61.82 | Train Loss 0.7915\n",
            "Epoch  1 | Batch 3090/8132 | Time/Batch(ms) 62.19 | Train Loss 0.7712\n",
            "Epoch  1 | Batch 3120/8132 | Time/Batch(ms) 62.76 | Train Loss 0.8549\n",
            "Epoch  1 | Batch 3150/8132 | Time/Batch(ms) 65.54 | Train Loss 0.8772\n",
            "Epoch  1 | Batch 3180/8132 | Time/Batch(ms) 63.67 | Train Loss 0.8240\n",
            "Epoch  1 | Batch 3210/8132 | Time/Batch(ms) 64.44 | Train Loss 0.7700\n",
            "Epoch  1 | Batch 3240/8132 | Time/Batch(ms) 63.31 | Train Loss 0.8788\n",
            "Epoch  1 | Batch 3270/8132 | Time/Batch(ms) 62.16 | Train Loss 0.8748\n",
            "Epoch  1 | Batch 3300/8132 | Time/Batch(ms) 62.55 | Train Loss 0.7324\n",
            "Epoch  1 | Batch 3330/8132 | Time/Batch(ms) 61.10 | Train Loss 0.9194\n",
            "Epoch  1 | Batch 3360/8132 | Time/Batch(ms) 60.67 | Train Loss 0.8944\n",
            "Epoch  1 | Batch 3390/8132 | Time/Batch(ms) 61.74 | Train Loss 0.7597\n",
            "Epoch  1 | Batch 3420/8132 | Time/Batch(ms) 61.37 | Train Loss 0.8221\n",
            "Epoch  1 | Batch 3450/8132 | Time/Batch(ms) 61.55 | Train Loss 0.7066\n",
            "Epoch  1 | Batch 3480/8132 | Time/Batch(ms) 61.78 | Train Loss 1.0613\n",
            "Epoch  1 | Batch 3510/8132 | Time/Batch(ms) 62.56 | Train Loss 0.9161\n",
            "Epoch  1 | Batch 3540/8132 | Time/Batch(ms) 62.25 | Train Loss 0.8810\n",
            "Epoch  1 | Batch 3570/8132 | Time/Batch(ms) 61.87 | Train Loss 0.9867\n",
            "Epoch  1 | Batch 3600/8132 | Time/Batch(ms) 61.54 | Train Loss 0.7440\n",
            "Epoch  1 | Batch 3630/8132 | Time/Batch(ms) 61.66 | Train Loss 0.7431\n",
            "Epoch  1 | Batch 3660/8132 | Time/Batch(ms) 62.77 | Train Loss 0.8152\n",
            "Epoch  1 | Batch 3690/8132 | Time/Batch(ms) 62.25 | Train Loss 0.5993\n",
            "Epoch  1 | Batch 3720/8132 | Time/Batch(ms) 61.61 | Train Loss 0.7944\n",
            "Epoch  1 | Batch 3750/8132 | Time/Batch(ms) 62.36 | Train Loss 0.8307\n",
            "Epoch  1 | Batch 3780/8132 | Time/Batch(ms) 61.92 | Train Loss 0.9052\n",
            "Epoch  1 | Batch 3810/8132 | Time/Batch(ms) 61.70 | Train Loss 0.8863\n",
            "Epoch  1 | Batch 3840/8132 | Time/Batch(ms) 61.95 | Train Loss 0.7618\n",
            "Epoch  1 | Batch 3870/8132 | Time/Batch(ms) 62.73 | Train Loss 0.8731\n",
            "Epoch  1 | Batch 3900/8132 | Time/Batch(ms) 62.21 | Train Loss 0.9013\n",
            "Epoch  1 | Batch 3930/8132 | Time/Batch(ms) 62.81 | Train Loss 0.7310\n",
            "Epoch  1 | Batch 3960/8132 | Time/Batch(ms) 63.38 | Train Loss 0.7300\n",
            "Epoch  1 | Batch 3990/8132 | Time/Batch(ms) 63.11 | Train Loss 0.9471\n",
            "Epoch  1 | Batch 4020/8132 | Time/Batch(ms) 62.40 | Train Loss 0.7104\n",
            "Epoch  1 | Batch 4050/8132 | Time/Batch(ms) 63.32 | Train Loss 0.8680\n",
            "Epoch  1 | Batch 4080/8132 | Time/Batch(ms) 63.34 | Train Loss 0.7927\n",
            "Epoch  1 | Batch 4110/8132 | Time/Batch(ms) 62.97 | Train Loss 0.7089\n",
            "Epoch  1 | Batch 4140/8132 | Time/Batch(ms) 62.98 | Train Loss 0.7717\n",
            "Epoch  1 | Batch 4170/8132 | Time/Batch(ms) 62.80 | Train Loss 0.6930\n",
            "Epoch  1 | Batch 4200/8132 | Time/Batch(ms) 64.41 | Train Loss 0.6943\n",
            "Epoch  1 | Batch 4230/8132 | Time/Batch(ms) 63.74 | Train Loss 0.6955\n",
            "Epoch  1 | Batch 4260/8132 | Time/Batch(ms) 64.79 | Train Loss 0.8787\n",
            "Epoch  1 | Batch 4290/8132 | Time/Batch(ms) 63.57 | Train Loss 0.8274\n",
            "Epoch  1 | Batch 4320/8132 | Time/Batch(ms) 63.51 | Train Loss 0.8088\n",
            "Epoch  1 | Batch 4350/8132 | Time/Batch(ms) 63.08 | Train Loss 0.8498\n",
            "Epoch  1 | Batch 4380/8132 | Time/Batch(ms) 62.84 | Train Loss 0.7919\n",
            "Epoch  1 | Batch 4410/8132 | Time/Batch(ms) 63.56 | Train Loss 0.8739\n",
            "Epoch  1 | Batch 4440/8132 | Time/Batch(ms) 63.26 | Train Loss 1.0117\n",
            "Epoch  1 | Batch 4470/8132 | Time/Batch(ms) 63.74 | Train Loss 0.8292\n",
            "Epoch  1 | Batch 4500/8132 | Time/Batch(ms) 63.84 | Train Loss 1.0500\n",
            "Epoch  1 | Batch 4530/8132 | Time/Batch(ms) 63.07 | Train Loss 0.8109\n",
            "Epoch  1 | Batch 4560/8132 | Time/Batch(ms) 63.84 | Train Loss 0.8220\n",
            "Epoch  1 | Batch 4590/8132 | Time/Batch(ms) 63.38 | Train Loss 0.8160\n",
            "Epoch  1 | Batch 4620/8132 | Time/Batch(ms) 63.82 | Train Loss 0.7389\n",
            "Epoch  1 | Batch 4650/8132 | Time/Batch(ms) 63.49 | Train Loss 0.6360\n",
            "Epoch  1 | Batch 4680/8132 | Time/Batch(ms) 64.78 | Train Loss 0.8808\n",
            "Epoch  1 | Batch 4710/8132 | Time/Batch(ms) 63.76 | Train Loss 0.7528\n",
            "Epoch  1 | Batch 4740/8132 | Time/Batch(ms) 64.79 | Train Loss 0.6692\n",
            "Epoch  1 | Batch 4770/8132 | Time/Batch(ms) 64.58 | Train Loss 0.9980\n",
            "Epoch  1 | Batch 4800/8132 | Time/Batch(ms) 64.76 | Train Loss 0.6285\n",
            "Epoch  1 | Batch 4830/8132 | Time/Batch(ms) 64.49 | Train Loss 0.8045\n",
            "Epoch  1 | Batch 4860/8132 | Time/Batch(ms) 64.14 | Train Loss 0.7435\n",
            "Epoch  1 | Batch 4890/8132 | Time/Batch(ms) 66.67 | Train Loss 0.8034\n",
            "Epoch  1 | Batch 4920/8132 | Time/Batch(ms) 64.25 | Train Loss 0.8731\n",
            "Epoch  1 | Batch 4950/8132 | Time/Batch(ms) 65.10 | Train Loss 0.7794\n",
            "Epoch  1 | Batch 4980/8132 | Time/Batch(ms) 65.86 | Train Loss 0.8075\n",
            "Epoch  1 | Batch 5010/8132 | Time/Batch(ms) 65.11 | Train Loss 0.7552\n",
            "Epoch  1 | Batch 5040/8132 | Time/Batch(ms) 66.65 | Train Loss 0.7135\n",
            "Epoch  1 | Batch 5070/8132 | Time/Batch(ms) 65.28 | Train Loss 0.8086\n",
            "Epoch  1 | Batch 5100/8132 | Time/Batch(ms) 66.03 | Train Loss 0.6198\n",
            "Epoch  1 | Batch 5130/8132 | Time/Batch(ms) 65.06 | Train Loss 0.9455\n",
            "Epoch  1 | Batch 5160/8132 | Time/Batch(ms) 64.41 | Train Loss 0.7364\n",
            "Epoch  1 | Batch 5190/8132 | Time/Batch(ms) 63.45 | Train Loss 0.8365\n",
            "Epoch  1 | Batch 5220/8132 | Time/Batch(ms) 63.07 | Train Loss 0.6916\n",
            "Epoch  1 | Batch 5250/8132 | Time/Batch(ms) 64.32 | Train Loss 0.7123\n",
            "Epoch  1 | Batch 5280/8132 | Time/Batch(ms) 62.19 | Train Loss 0.8426\n",
            "Epoch  1 | Batch 5310/8132 | Time/Batch(ms) 61.68 | Train Loss 0.8147\n",
            "Epoch  1 | Batch 5340/8132 | Time/Batch(ms) 61.49 | Train Loss 0.7820\n",
            "Epoch  1 | Batch 5370/8132 | Time/Batch(ms) 62.23 | Train Loss 0.8800\n",
            "Epoch  1 | Batch 5400/8132 | Time/Batch(ms) 61.34 | Train Loss 0.8757\n",
            "Epoch  1 | Batch 5430/8132 | Time/Batch(ms) 62.89 | Train Loss 0.7898\n",
            "Epoch  1 | Batch 5460/8132 | Time/Batch(ms) 63.15 | Train Loss 0.8338\n",
            "Epoch  1 | Batch 5490/8132 | Time/Batch(ms) 62.19 | Train Loss 0.8843\n",
            "Epoch  1 | Batch 5520/8132 | Time/Batch(ms) 62.35 | Train Loss 0.7968\n",
            "Epoch  1 | Batch 5550/8132 | Time/Batch(ms) 61.74 | Train Loss 0.7710\n",
            "Epoch  1 | Batch 5580/8132 | Time/Batch(ms) 62.12 | Train Loss 0.6144\n",
            "Epoch  1 | Batch 5610/8132 | Time/Batch(ms) 63.42 | Train Loss 0.6780\n",
            "Epoch  1 | Batch 5640/8132 | Time/Batch(ms) 63.18 | Train Loss 0.8143\n",
            "Epoch  1 | Batch 5670/8132 | Time/Batch(ms) 63.51 | Train Loss 0.7660\n",
            "Epoch  1 | Batch 5700/8132 | Time/Batch(ms) 62.84 | Train Loss 0.7333\n",
            "Epoch  1 | Batch 5730/8132 | Time/Batch(ms) 63.02 | Train Loss 0.8413\n",
            "Epoch  1 | Batch 5760/8132 | Time/Batch(ms) 63.87 | Train Loss 0.8946\n",
            "Epoch  1 | Batch 5790/8132 | Time/Batch(ms) 62.52 | Train Loss 0.6839\n",
            "Epoch  1 | Batch 5820/8132 | Time/Batch(ms) 63.05 | Train Loss 0.7074\n",
            "Epoch  1 | Batch 5850/8132 | Time/Batch(ms) 62.62 | Train Loss 0.8434\n",
            "Epoch  1 | Batch 5880/8132 | Time/Batch(ms) 62.09 | Train Loss 0.6616\n",
            "Epoch  1 | Batch 5910/8132 | Time/Batch(ms) 62.88 | Train Loss 0.8600\n",
            "Epoch  1 | Batch 5940/8132 | Time/Batch(ms) 63.37 | Train Loss 0.7241\n",
            "Epoch  1 | Batch 5970/8132 | Time/Batch(ms) 61.68 | Train Loss 0.8012\n",
            "Epoch  1 | Batch 6000/8132 | Time/Batch(ms) 62.51 | Train Loss 0.8442\n",
            "Epoch  1 | Batch 6030/8132 | Time/Batch(ms) 62.15 | Train Loss 0.7929\n",
            "Epoch  1 | Batch 6060/8132 | Time/Batch(ms) 62.38 | Train Loss 0.7254\n",
            "Epoch  1 | Batch 6090/8132 | Time/Batch(ms) 62.81 | Train Loss 0.6940\n",
            "Epoch  1 | Batch 6120/8132 | Time/Batch(ms) 62.19 | Train Loss 0.7418\n",
            "Epoch  1 | Batch 6150/8132 | Time/Batch(ms) 62.12 | Train Loss 0.7341\n",
            "Epoch  1 | Batch 6180/8132 | Time/Batch(ms) 62.04 | Train Loss 0.7459\n",
            "Epoch  1 | Batch 6210/8132 | Time/Batch(ms) 62.26 | Train Loss 0.6809\n",
            "Epoch  1 | Batch 6240/8132 | Time/Batch(ms) 61.43 | Train Loss 0.8052\n",
            "Epoch  1 | Batch 6270/8132 | Time/Batch(ms) 62.95 | Train Loss 0.7442\n",
            "Epoch  1 | Batch 6300/8132 | Time/Batch(ms) 62.42 | Train Loss 0.7703\n",
            "Epoch  1 | Batch 6330/8132 | Time/Batch(ms) 62.51 | Train Loss 0.7199\n",
            "Epoch  1 | Batch 6360/8132 | Time/Batch(ms) 61.87 | Train Loss 0.7895\n",
            "Epoch  1 | Batch 6390/8132 | Time/Batch(ms) 62.56 | Train Loss 0.6770\n",
            "Epoch  1 | Batch 6420/8132 | Time/Batch(ms) 63.37 | Train Loss 0.8718\n",
            "Epoch  1 | Batch 6450/8132 | Time/Batch(ms) 63.02 | Train Loss 0.7362\n",
            "Epoch  1 | Batch 6480/8132 | Time/Batch(ms) 62.50 | Train Loss 0.6617\n",
            "Epoch  1 | Batch 6510/8132 | Time/Batch(ms) 63.22 | Train Loss 0.8326\n",
            "Epoch  1 | Batch 6540/8132 | Time/Batch(ms) 63.34 | Train Loss 0.7395\n",
            "Epoch  1 | Batch 6570/8132 | Time/Batch(ms) 61.89 | Train Loss 0.7038\n",
            "Epoch  1 | Batch 6600/8132 | Time/Batch(ms) 61.80 | Train Loss 0.8054\n",
            "Epoch  1 | Batch 6630/8132 | Time/Batch(ms) 62.13 | Train Loss 0.9174\n",
            "Epoch  1 | Batch 6660/8132 | Time/Batch(ms) 61.97 | Train Loss 0.7554\n",
            "Epoch  1 | Batch 6690/8132 | Time/Batch(ms) 61.72 | Train Loss 0.7393\n",
            "Epoch  1 | Batch 6720/8132 | Time/Batch(ms) 61.43 | Train Loss 0.8460\n",
            "Epoch  1 | Batch 6750/8132 | Time/Batch(ms) 61.97 | Train Loss 0.7186\n",
            "Epoch  1 | Batch 6780/8132 | Time/Batch(ms) 61.76 | Train Loss 0.6878\n",
            "Epoch  1 | Batch 6810/8132 | Time/Batch(ms) 63.25 | Train Loss 0.7775\n",
            "Epoch  1 | Batch 6840/8132 | Time/Batch(ms) 63.74 | Train Loss 0.7852\n",
            "Epoch  1 | Batch 6870/8132 | Time/Batch(ms) 63.10 | Train Loss 0.7899\n",
            "Epoch  1 | Batch 6900/8132 | Time/Batch(ms) 64.26 | Train Loss 0.6394\n",
            "Epoch  1 | Batch 6930/8132 | Time/Batch(ms) 64.24 | Train Loss 0.9964\n",
            "Epoch  1 | Batch 6960/8132 | Time/Batch(ms) 64.33 | Train Loss 0.6829\n",
            "Epoch  1 | Batch 6990/8132 | Time/Batch(ms) 64.45 | Train Loss 0.7462\n",
            "Epoch  1 | Batch 7020/8132 | Time/Batch(ms) 64.96 | Train Loss 0.8536\n",
            "Epoch  1 | Batch 7050/8132 | Time/Batch(ms) 64.83 | Train Loss 0.7309\n",
            "Epoch  1 | Batch 7080/8132 | Time/Batch(ms) 64.07 | Train Loss 0.6732\n",
            "Epoch  1 | Batch 7110/8132 | Time/Batch(ms) 64.94 | Train Loss 0.7245\n",
            "Epoch  1 | Batch 7140/8132 | Time/Batch(ms) 64.80 | Train Loss 0.7398\n",
            "Epoch  1 | Batch 7170/8132 | Time/Batch(ms) 63.99 | Train Loss 0.7344\n",
            "Epoch  1 | Batch 7200/8132 | Time/Batch(ms) 64.01 | Train Loss 0.7828\n",
            "Epoch  1 | Batch 7230/8132 | Time/Batch(ms) 63.40 | Train Loss 0.7303\n",
            "Epoch  1 | Batch 7260/8132 | Time/Batch(ms) 64.08 | Train Loss 0.7860\n",
            "Epoch  1 | Batch 7290/8132 | Time/Batch(ms) 64.30 | Train Loss 0.9741\n",
            "Epoch  1 | Batch 7320/8132 | Time/Batch(ms) 63.97 | Train Loss 0.7281\n",
            "Epoch  1 | Batch 7350/8132 | Time/Batch(ms) 63.04 | Train Loss 0.7594\n",
            "Epoch  1 | Batch 7380/8132 | Time/Batch(ms) 63.21 | Train Loss 0.7672\n",
            "Epoch  1 | Batch 7410/8132 | Time/Batch(ms) 62.58 | Train Loss 0.8003\n",
            "Epoch  1 | Batch 7440/8132 | Time/Batch(ms) 62.95 | Train Loss 0.8571\n",
            "Epoch  1 | Batch 7470/8132 | Time/Batch(ms) 62.72 | Train Loss 0.7912\n",
            "Epoch  1 | Batch 7500/8132 | Time/Batch(ms) 62.67 | Train Loss 0.7963\n",
            "Epoch  1 | Batch 7530/8132 | Time/Batch(ms) 62.66 | Train Loss 0.5561\n",
            "Epoch  1 | Batch 7560/8132 | Time/Batch(ms) 62.36 | Train Loss 0.7389\n",
            "Epoch  1 | Batch 7590/8132 | Time/Batch(ms) 61.94 | Train Loss 0.6554\n",
            "Epoch  1 | Batch 7620/8132 | Time/Batch(ms) 61.58 | Train Loss 0.7072\n",
            "Epoch  1 | Batch 7650/8132 | Time/Batch(ms) 61.45 | Train Loss 0.9071\n",
            "Epoch  1 | Batch 7680/8132 | Time/Batch(ms) 62.53 | Train Loss 0.7810\n",
            "Epoch  1 | Batch 7710/8132 | Time/Batch(ms) 62.07 | Train Loss 0.8155\n",
            "Epoch  1 | Batch 7740/8132 | Time/Batch(ms) 62.35 | Train Loss 0.5974\n",
            "Epoch  1 | Batch 7770/8132 | Time/Batch(ms) 62.57 | Train Loss 0.6786\n",
            "Epoch  1 | Batch 7800/8132 | Time/Batch(ms) 61.98 | Train Loss 0.7330\n",
            "Epoch  1 | Batch 7830/8132 | Time/Batch(ms) 62.27 | Train Loss 0.6711\n",
            "Epoch  1 | Batch 7860/8132 | Time/Batch(ms) 62.61 | Train Loss 0.8620\n",
            "Epoch  1 | Batch 7890/8132 | Time/Batch(ms) 61.75 | Train Loss 0.8159\n",
            "Epoch  1 | Batch 7920/8132 | Time/Batch(ms) 62.21 | Train Loss 0.7625\n",
            "Epoch  1 | Batch 7950/8132 | Time/Batch(ms) 61.54 | Train Loss 0.8509\n",
            "Epoch  1 | Batch 7980/8132 | Time/Batch(ms) 62.30 | Train Loss 0.7643\n",
            "Epoch  1 | Batch 8010/8132 | Time/Batch(ms) 64.03 | Train Loss 0.7436\n",
            "Epoch  1 | Batch 8040/8132 | Time/Batch(ms) 63.49 | Train Loss 0.7761\n",
            "Epoch  1 | Batch 8070/8132 | Time/Batch(ms) 63.96 | Train Loss 0.9197\n",
            "Epoch  1 | Batch 8100/8132 | Time/Batch(ms) 62.88 | Train Loss 0.7471\n",
            "Epoch  1 | Batch 8130/8132 | Time/Batch(ms) 62.31 | Train Loss 0.8184\n",
            "--------------------------------------------------\n",
            "Epoch  1 | Time 557.7914 sec | Valid Loss 0.7833 | Test Loss 0.8298\n",
            "--------------------------------------------------\n",
            "Saved model at pre_trained_models/mult.pt!\n",
            "MAE:  0.82983476\n",
            "Correlation Coefficient:  0.46611261298026324\n",
            "mult_acc_7:  0.3663579582166703\n",
            "mult_acc_5:  0.36894249407710533\n",
            "F1 score:  0.7362482029622642\n",
            "Accuracy:  0.7361878453038674\n",
            "--------------------------------------------------\n",
            "[Press Any Key to start another run]Traceback (most recent call last):\n",
            "  File \"main.py\", line 163, in <module>\n",
            "    test_loss = train.initiate(hyp_params, train_loader, valid_loader, test_loader)\n",
            "  File \"/content/Multimodal-Transformer/src/train.py\", line 72, in initiate\n",
            "    return train_model(settings, hyp_params, train_loader, valid_loader, test_loader)\n",
            "  File \"/content/Multimodal-Transformer/src/train.py\", line 278, in train_model\n",
            "    input('[Press Any Key to start another run]')\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/Multimodal-Transformer"
      ],
      "metadata": {
        "id": "aomAmYoRxUBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = \"mosei_senti\"\n",
        "# data_path = \"./data\"\n",
        "# split_type = \"train\"\n",
        "\n",
        "# import numpy as np\n",
        "# from torch.utils.data.dataset import Dataset\n",
        "# import pickle\n",
        "# import os\n",
        "# from scipy import signal\n",
        "# import torch\n",
        "\n",
        "# dataset_path = os.path.join(data_path, f\"{dataset}_data.pkl\")\n",
        "# print(dataset_path)\n",
        "# dataset = pickle.load(open(dataset_path, 'rb'))\n",
        "\n",
        "# vision = torch.tensor(dataset[split_type]['vision'].astype(np.float32)).cpu().detach()\n",
        "# print(vision.shape)\n",
        "# text = torch.tensor(dataset[split_type]['text'].astype(np.float32)).cpu().detach()\n",
        "# print(text.shape)\n",
        "# audio = dataset[split_type]['audio'].astype(np.float32)\n",
        "# audio[audio == -np.inf] = 0\n",
        "# audio = torch.tensor(audio).cpu().detach()\n",
        "# print(audio.shape)\n",
        "# labels = torch.tensor(dataset[split_type]['labels'].astype(np.float32)).cpu().detach()\n",
        "# print(labels.shape)\n",
        "# meta = dataset[split_type]['id'] if 'id' in dataset[split_type].keys() else None\n",
        "# print(meta)\n",
        "# data = \"mosei_senti\"\n",
        "\n",
        "# n_modalities = 3\n",
        "\n",
        "# def get_item(index):\n",
        "#   X = (index, text[index], audio[index], vision[index])\n",
        "#   Y = labels[index]\n",
        "#   META = (0,0,0) if meta is None else (meta[index][0], meta[index][1], meta[index][2])\n",
        "#   if data == 'mosi':\n",
        "#       META = (meta[index][0].decode('UTF-8'), meta[index][1].decode('UTF-8'), meta[index][2].decode('UTF-8'))\n",
        "#   if data == 'iemocap':\n",
        "#       Y = torch.argmax(Y, dim=-1)\n",
        "#   return X, Y, META\n",
        "\n",
        "# get_item(0)"
      ],
      "metadata": {
        "id": "lXf_gRkN-5qA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}